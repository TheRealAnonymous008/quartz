* A naive approach in this regard would be to directly apply the techniques from [[Function Approximation in Reinforcement Learning|Function Approximation]] and [[Policy Gradient Methods|PGMs]] to each agent without making use of joint information. 
* To facilitate faster learning, it may help to apply [[Value Decomposition in MARL|Value Decomposition techniques]]

# Independent Learning 
## Independent DQN 
* An extension of [[Off Policy Prediction and Control with Approximation#DQN|DQN]] but applied to multiple agents. Here, each agent optimizes the loss function using its local observation history $\mathcal{B}$. 
  
  $$
  \mathcal{L}(\theta) = \frac{1}{B} \sum_{(h^t_i,a^t_i, r^t_i, h^{t+1}_i)\in \mathcal{B}} \bigg(r_i^t +\gamma\max_{a_i\in A_i} Q(h_i^{t+1}, a_i ; \overline\theta_i) - Q(h_i^t, a_i^t; \theta_i ) \bigg)
  $$
* One challenge is that *the replay buffer has to be modified* since agents in IL do not distinguish between the non-stationarity caused by the environment and that by the other agents. 
	* This means that the agent can receive different rewards for performing the same action at the same state 
	* Additionally, because the agents' policies update, the information in the replay buffer can become outdated .
	* This can be solved in a few ways: 
		* By *making the replay buffer smaller* 
		* Using [[Importance Sampling]] to re-weight the experiences in the buffer to account for changing policies and correcting the non-stationarity. 
		* **Hysteretic Q-Learning** uses a smaller learning rate so that decreasing estimates will reduce the stochasticity of the other agent's policies 
		* Use **leniency** ignores decreasing updates of action-value estimates with a given probability that decreases throughout training. 

![[IDQN.png]]
<figcaption> IDQN. Image taken from Albrecht, Christianos and Schafer </figcaption>

## IPGM 
* Compute the gradient with respect to its own policy parameters and then apply the Policy Gradient Theorem 
* *On policy algorithms have an advantage over off-policy approaches since the policy gradient is computed based on recent experiences generated by the current policy*.

![[Independent Reinforce.png]]
<figcaption> Independent REINFORCE. Image taken from Albrecht, Christianos and Schafer</figcaption>

* It can be made asynchronous (such as [[Basic Policy Gradient Methods#A2C - Advantage Actor-Critic|A2C]]). 
![[Independent A2C.png]]
<figcaption> Independent A2C. Image taken from Albrecht, Christianos and Schafer</figcaption>

* We can also use [[Trust Region Policies#PPO|PPO]]. 

# Multi-Agent PGMs 
* The **Multi-Agent Policy Gradient Theorem** extends the regular Policy Gradient Theorem by considering all of the agent's policies. 

$$
\nabla_{\phi_i} J(\theta_i) \propto \mathbb{E}_{\hat{h}\sim P(h\mid \pi), a_i \sim \pi_i, a_{-i}\sim \pi_{-i}} \bigg[Q_i^\pi (\hat{h}, \braket{a_i,a_{-1}}) \nabla _{\phi_i} \log \pi _i \bigg(a_i\mid h_i=\sigma_i(\hat{h}) ; \phi_i\bigg) \bigg]
$$

* We can make use of a **centralized critic**  [^mapgm_1] that is conditioned on information beyond local information. 
	* The centralized critic is optimized using [[Off Policy Prediction and Control with Approximation#Semi-Gradient Methods|Semi-Gradient descent]]. 
	* At minimum, we need to condition the critic on the agent's observation history $h_i^t$. 
	* Any additional information may introduce noise, and increase variance, so it is subject to something akin to the [[Statistical Estimators#Bias-Variance Tradeoff|bias-variance tradeoff]].
	* In practice, we use *state history + local observation history*.
	* This makes it more robust against non-stationarity. 
	* Any [[#Independent Learning]] algorithm can be instantiated with a centralized critic to learn a value function.

![[CA2C.png]]

<figcaption> Centralized A2C. Image taken from Albrecht, Christianos and Schafer </figcaption>

[^mapgm_1]: We don't need to define a decentralized critic since during inference the critic is never used. 
## Action Value Critics and COMA
* *The centralized critic can make use of action-values* $Q$ as well. which is conditioned on local observation history, centralized information  information, and the joint action.
	* The gradient update for the policy then becomes  
	  
	  $$
	  \nabla_{\phi_i} J (\phi_i) = \mathbb{E}_{a^t\sim \pi}\bigg[Q(h_i^t ,z^t, \braket{a_i^t, a_{-i}^t}) \ \nabla_{\phi_i} \log\pi_i (a_i^t \mid h_i^t;\phi_i)\bigg]
	  $$

	* *We make use of on-policy data rather than off-policy data*.
	* In practice, rather than have many action-values for each joint action (with high dimensionality at scale), we instead output the action value given the current joint action (which scales linearly to the number of agents)

* *We can use action-value critics for the multi-agent credit assignment problem* using **difference rewards** to approximate the difference in reward if the agent took action $\bar{a}_i$ instead. We call $\bar{a}_i$ the **default action**
  
  $$
  d_i = R_i(s, \braket{a_i,a_{-i}}) - R_i (s, \bar{a}_i, a_{-i})
  $$
  
	* In practice, determining the default action is difficult, so we use the **aristocratic utility** which measures if the chosen action performs better than a random action sampled from the policy 
	  
	  $$
	  d_i = R(s,\braket{a_i,a_{-i}}) - \mathbb{E}_{a_i'\sim \pi_i} \big[R_i (s, \braket{a_i', a_{-i}}) \big]
	  $$

* **Counterfactual Multi-agent Policy Gradient (COMA)** estimates the advantage using a counterfactual baseline computed using the aristocratic utility. *This enables more efficient computation at the cost of higher variance* 
  
  $$
  \text{Adv}_i(h_i, z, a) = Q(h_i ,z,a;\theta) - \sum_{a_i'\in A_i} \pi (a_i'\mid h_i;\phi_i) \ Q(h_i, z, \braket{a_i',a_{-i}}; \theta) 
  $$

## Pareto-AC
* **Pareto-AC** is an approach where the goal is to learn a Pareto-optimal equilibrium in no-conflict games. This is done by *incorporating the inductive bias that all other agents prefer the same outcome*. That is, assume all other agents follow the policy $\pi_{-i}^+$ which maximizes its reward (or in training, its $Q$-value). That is 
  
  $$
  \pi_{-i}^+ \in \underset{\pi_{-i}}{\text{argmax}} \ U_i(\pi_i,\pi_{-i}) = \underset{a_{-i}}{\text{argmax}} \ Q(h_i^t, z^t, \braket{a_i^t, a_{-i}})
  $$

* The loss function for the policy turns out to be 
  $$
  \mathcal{L}(\phi_i) = -\mathbb{E}_{a'_i \sim \pi_i, a_{-i}^t \sim \pi_{-i}^t} \bigg[\log\pi(a_i^t\mid h_i^t ; \phi_i) \ \bigg(Q^{\pi^+}  (h_i^t, z^t, \braket{a_i^t, a_{-i}^t}; \theta _i^q - V^{\pi^+} (h_i^t, z^t ; \theta_i^v)    \bigg) \bigg]
  $$

* Pareto-AC suffers from inefficiencies as agent count increases

# Links 
* [[MARL Problem Statement]]
* [[MARL from a Game Theoretic Perspective]]

* [[Multi-Agent Reinforcement Learning -- Foundations and Modern Approaches by Albrecht, Christianos and Schafer|Albrecht, Christianos, and Schafer]] - Ch. 9
