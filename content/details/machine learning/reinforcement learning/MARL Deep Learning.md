* A naive approach in this regard would be to directly apply the techniques from [[Function Approximation in Reinforcement Learning|Function Approximation]] and [[Policy Gradient Methods|PGMs]] to each agent without making use of joint information. 

# Independent Learning 
## Independent DQN 
* An extension of [[Off Policy Prediction and Control with Approximation#DQN|DQN]] but applied to multiple agents. Here, each agent optimizes the loss function using its local observation history from batch $\mathcal{B}$. 
  
  $$
  \mathcal{L}(\theta) = \frac{1}{|\mathcal{B}|} \sum_{(h^t_i,a^t_i, r^t_i, h^{t+1}_i)\in \mathcal{B}} \bigg(r_i^t +\gamma\max_{a_i\in A_i} Q(h_i^{t+1}, a_i ; \overline\theta_i) - Q(h_i^t, a_i^t; \theta_i ) \bigg)
  $$
* One challenge is that *the replay buffer has to be modified* since agents in IL do not distinguish between the non-stationarity caused by the environment and that by the other agents. 
	* This means that the agent can receive different rewards for performing the same action at the same state 
	* Additionally, because the agents' policies update, the information in the replay buffer can become outdated .
	* This can be solved in a few ways: 
		* By *making the replay buffer smaller* 
		* Using [[Importance Sampling]] to re-weight the experiences in the buffer to account for changing policies and correcting the non-stationarity. 
		* **Hysteretic Q-Learning** uses a smaller learning rate so that decreasing estimates will reduce the stochasticity of the other agent's policies 
		* Use **leniency** ignores decreasing updates of action-value estimates with a given probability that decreases throughout training. 

![[IDQN.png]]
<figcaption> IDQN. Image taken from Albrecht, Christianos and Schafer </figcaption>

## IPGM 
* Compute the gradient with respect to its own policy parameters and then apply the Policy Gradient Theorem 
* *On policy algorithms have an advantage over off-policy approaches since the policy gradient is computed based on recent experiences generated by the current policy*.

![[Independent Reinforce.png]]
<figcaption> Independent REINFORCE. Image taken from Albrecht, Christianos and Schafer</figcaption>

* It can be made asynchronous (such as [[Basic Policy Gradient Methods#A2C - Advantage Actor-Critic|A2C]]). 
![[Independent A2C.png]]
<figcaption> Independent A2C. Image taken from Albrecht, Christianos and Schafer</figcaption>

* We can also use [[Trust Region Policies#PPO|PPO]]. 

# Multi-Agent PGMs 
* The **Multi-Agent Policy Gradient Theorem** extends the regular Policy Gradient Theorem by considering all of the agent's policies. 

$$
\nabla_{\phi_i} J(\theta_i) \propto \mathbb{E}_{\hat{h}\sim P(h\mid \pi), a_i \sim \pi_i, a_{-i}\sim \pi_{-i}} \bigg[Q_i^\pi (\hat{h}, \braket{a_i,a_{-1}}) \nabla _{\phi_i} \log \pi _i \bigg(a_i\mid h_i=\sigma_i(\hat{h}) ; \phi_i\bigg) \bigg]
$$

* We can make use of a **centralized critic**  [^mapgm_1] that is conditioned on information beyond local information. 
	* The centralized critic is optimized using [[Off Policy Prediction and Control with Approximation#Semi-Gradient Methods|Semi-Gradient descent]]. 
	* At minimum, we need to condition the critic on the agent's observation history $h_i^t$. 
	* Any additional information may introduce noise, and increase variance, so it is subject to something akin to the [[Statistical Estimators#Bias-Variance Tradeoff|bias-variance tradeoff]].
	* In practice, we use *state history + local observation history*.
	* This makes it more robust against non-stationarity. 
	* Any [[#Independent Learning]] algorithm can be instantiated with a centralized critic to learn a value function.

![[CA2C.png]]

<figcaption> Centralized A2C. Image taken from Albrecht, Christianos and Schafer </figcaption>

[^mapgm_1]: We don't need to define a decentralized critic since during inference the critic is never used. 
## Action Value Critics and COMA
* *The centralized critic can make use of action-values* $Q$ as well. which is conditioned on local observation history, centralized information  information, and the joint action.
	* The gradient update for the policy then becomes  
	  
	  $$
	  \nabla_{\phi_i} J (\phi_i) = \mathbb{E}_{a^t\sim \pi}\bigg[Q(h_i^t ,z^t, \braket{a_i^t, a_{-i}^t}) \ \nabla_{\phi_i} \log\pi_i (a_i^t \mid h_i^t;\phi_i)\bigg]
	  $$

	* *We make use of on-policy data rather than off-policy data*.
	* In practice, rather than have many action-values for each joint action (with high dimensionality at scale), we instead output the action value given the current joint action (which scales linearly to the number of agents)

* *We can use action-value critics for the multi-agent credit assignment problem* using **difference rewards** to approximate the difference in reward if the agent took action $\bar{a}_i$ instead. We call $\bar{a}_i$ the **default action**
  
  $$
  d_i = R_i(s, \braket{a_i,a_{-i}}) - R_i (s, \bar{a}_i, a_{-i})
  $$
  
	* In practice, determining the default action is difficult, so we use the **aristocratic utility** which measures if the chosen action performs better than a random action sampled from the policy 
	  
	  $$
	  d_i = R(s,\braket{a_i,a_{-i}}) - \mathbb{E}_{a_i'\sim \pi_i} \big[R_i (s, \braket{a_i', a_{-i}}) \big]
	  $$

* **Counterfactual Multi-agent Policy Gradient (COMA)** estimates the advantage using a counterfactual baseline computed using the aristocratic utility. *This enables more efficient computation at the cost of higher variance* 
  
  $$
  \text{Adv}_i(h_i, z, a) = Q(h_i ,z,a;\theta) - \sum_{a_i'\in A_i} \pi (a_i'\mid h_i;\phi_i) \ Q(h_i, z, \braket{a_i',a_{-i}}; \theta) 
  $$

## Pareto-AC
* **Pareto-AC** is an approach where the goal is to learn a Pareto-optimal equilibrium in no-conflict games. This is done by *incorporating the inductive bias that all other agents prefer the same outcome*. That is, assume all other agents follow the policy $\pi_{-i}^+$ which maximizes its reward (or in training, its $Q$-value). That is 
  
  $$
  \pi_{-i}^+ \in \underset{\pi_{-i}}{\text{argmax}} \ U_i(\pi_i,\pi_{-i}) = \underset{a_{-i}}{\text{argmax}} \ Q(h_i^t, z^t, \braket{a_i^t, a_{-i}})
  $$

* The loss function for the policy turns out to be 
  $$
  \mathcal{L}(\phi_i) = -\mathbb{E}_{a'_i \sim \pi_i, a_{-i}^t \sim \pi_{-i}^t} \bigg[\log\pi(a_i^t\mid h_i^t ; \phi_i) \ \bigg(Q^{\pi^+}  (h_i^t, z^t, \braket{a_i^t, a_{-i}^t}; \theta _i^q - V^{\pi^+} (h_i^t, z^t ; \theta_i^v)    \bigg) \bigg]
  $$

* Pareto-AC suffers from inefficiencies as agent count increases

# Value Decomposition and Common-Reward Games 
* *Rationale*: Regular [[#Multi-Agent PGMs|PGM]]- based approaches are difficult because (1) it can be difficult to learn a centralized value function that scales with exponential action space; and (2) centralized value functions do not imply efficient decentralized execution.  [^valdec_1]
[^valdec]: One approach is to use the observation that not all agents interact with each other. 


* The goal is to decompose the following where $r$ is the common reward. 
  
  $$
  Q(h^t, z^t, a^t ; \theta) = \mathbb{E}\bigg[ \sum_{\tau=t}^\infty \gamma^{\tau-1} r^\tau \ \ \ \bigg\vert \ \ \ h^t, z^t, a^t  \bigg]
  $$
* A simpler **utility function** can be used for each agent, conditioned only on individual observation history and actions. We denote this with $Q(h_i,a_i; \theta_i)$

* The **Individual-Global-Max (IGM) property** states that greedy joint actions with respect to the centralized action-value function should be equal to the joint action composed of the greedy individual actions of all agents that maximize their individual utilities. 
	* More formally, define both forms of greedy actions as follows 
	  
	  $$
	  \begin{split}
	  A^\ast (h, z;\theta) &= \underset{a\in A}{\text{argmax}} \ Q(h,z,a;\theta) \\ 
	  A_i^\ast (h_i; \theta) &= \underset{a_i \in A_i}{\text{argmax}} \ Q(h_i, a_i; \theta_i)
	  \end{split}
	  $$

	* The IGM property is satisfied if the following holds for all histories $\hat{h}$ with joint observation histories $h=\sigma(\hat{h})$. individual observation histories $h_i=\sigma_i(\hat{h})$ and centralized information $z$.
	  
	  $$
	  \forall a =(a_1,\dots,a_n)\in A : a\in A^\ast (h,z;\theta ) \iff \forall i \in N: a_i \in A_i^\ast (h_i;\theta _i )
	  $$

	* *Because of IGM, we can have all agents follow a policy based on their local utility* (which is more efficient to compute)  and it would be equivalent to the original Centralized Value Function-based policy 
	* It also *helps establish reward credit* since the reward is "common" to all agents. 
	* *The IGM property is not necessarily satisfied for all environments*

## Linear Value Decomposition 
* *Assume a linear decomposition of common rewards*. That is 
  $$
  r_t = \sum_{i\in N} \overline{r}_i^t 
  $$
* The decomposition is as follows. It is guaranteed to satisfy the IGM property 
  $$
  Q(h^t, z^t ,a^t; \theta) = \sum_{i\in N} Q(h_i^t ,a_i^t; \theta_i )
  $$
* **Value Decomposition Networks (VDN)** maintain a replay buffer containing the experience of all agents and jointly optimizes the loss function 
  
  $$
  \begin{split}
  \mathcal{L} (\theta) &= \frac{1}{|\mathcal{B}|} \sum_{(h^t , a^t, r^t ,h^{t+1}) \in \mathcal{B}} \bigg(r^t + \gamma \max_{a\in A} \ Q(h^{t+1}, a;\bar{\theta}) - Q(h^t,a^t;\theta) \bigg)^2  \\ 
  
  Q(h^t,a^t;\theta) &= \sum_{i\in N} Q(h_i^t, a_i^t; \theta ) \\ 
  
  \max_{a\in A} Q (h^{t+1},a;\overline{\theta}) &= \sum_{i\in N} \max_{a_i\in A_i} \ Q(h_i^{t+1}, a; \overline{\theta}_i)
  \end{split}
  $$

![[VDN.png]]
<figcaption> VDN. Image taken from Albrecht, Christianos , and Schafer </figcaption>

## Monotonic Value Decomposition 
* Extends [[#Linear Value Decomposition]] to apply to cases when the contribution of each agent to the common reward is non-linear. 

* **QMIX** extends VDN by ensuring that the centralized action-value function is strictly monotonic with respect to individual utilities. That is 
  
  $$
  \forall i \in N, \forall a \in A: \frac{\partial  Q(h,z,a;\theta)}{\partial Q(h_i,a_i; \theta_i)} > 0 
  $$
	* In other words *the utility of any agent for its action must increase the decomposed centralized action value function*. 

* QMIX uses a mixing network which is simply a standard feedforward network $f_{\text{mix}}$ that combines individual utilities 
  
  $$
  Q (h,z,a,\theta) = f_\text{mix} \bigg(Q(h_1,a_1; \theta_1), \dots, Q(h_N, a_N; \theta_N)  \ ; \ \theta_{\text{mix}} \bigg)
  $$
	* To ensure monotonicity, we assume the mixing network only allows for positive weights. 
	* $\theta_{\text{mix}}$ is obtained using a [[Hypernetwork|hypernetworks]] $f_{\text{hyper}}$, which also receives $z$ as input. When it outputs $\theta_{\text{mix}}$, it applies an absolute value.  Because the hypernetwork receives $z$ as input, we add $z$ to the replay buffer. 
	* The loss function optimized is 
	  
$$
\mathcal{L}(\theta) = \frac{1}{|\mathcal B| } \sum_{(h^t,z^t,a^t,r^t, h^{t+1}, z^{t+1}) \in \mathcal{B}}  \bigg(r^t  +\gamma \max_{a\in A} Q(h^{t+1}, z^{t+1}, a;\overline{\theta})- Q(h^t,z^t,a^t;\theta)  \bigg)
$$

![[QMIX.png]]
<figcaption> QMIX. Image taken from Albrecht, Christianos and Schafer </figcaption>

## QTRAN
* Monotonic Value Decomposition only provides a sufficient condition but not a necessary condition.
* The following decomposition gives both sufficient and necessary (under affine transformations) conditions. 
  
  $$
  \sum_{i\in N} Q(h_i,a_i;\theta_i) - Q(h,z,a;\theta^q) + V(h,z;\theta^v)= 
  \begin{cases}
  0 & \text{if } a= a^\ast  \\
  \ge 0  & \text{otherwise}
  \end{cases}
  $$
  
  Where  $a^\ast$ is the greedy joint action , $Q(h,z,a;\theta^q)$ is the unrestricted centralized action-value function, and $V$ denotes a utility function 
  $$
  V(h,z;\theta^v) = \max_{a\in A} Q(h,z,a;\theta^q) - \sum_{i\in N} Q(h_i, a_i^\ast; \theta_i) 
  $$

* *$V$ is conditioned on centralized information since the individual utility functions of each agent may lack information* (especially if the environment is partially observable). 

* **QTRAN** trains a network that optimizes the following: 
	* For each agent, their individual utility functions $Q(h_i,a_i;\theta_i)$ 
	* A single network for the global utility function $V(h,z;\theta^v)$ 
	* A single network for the centralized action-value function $Q(h,z,a,\theta^q)$. 

* For the centralized action-value function the following loss function (a TD-error) is minimized 
  
$$
\mathcal{L}_{\text{id}} (\theta^q) = \frac{1}{|\mathcal{B}|} \sum_{(h^t, z^t,a^t,r^t,h^{t+1},z^{t+1}) \in \mathcal{B} }  \bigg(r^t + \gamma Q(h^{t+1}, z^{t+1}, a^{\ast t+1}; \overline{\theta}_q) - Q(h^t,z^t, a^t;\theta) \bigg)^2
$$
* For the utility functions, we minimize additional regularization terms given by the following (for the first case in the condition specified above where $a=a^\ast$)
  
  $$
  \mathcal{L}_{\text{opt}} (\{\theta_i\}_{i\in N}, \theta^v) = \frac{1}{|\mathcal{B}|} \sum_{(h^t,z^t,a^t, r^t, h^{t+1}, z^{t+1})\in\mathcal{B}} \bigg(\sum_{i\in N} Q(h_i^t,a_i^{\ast t}; \theta_i) - Q(h^t,z^t, a^{\ast t}; \theta^q) + V(h^t,z^t; \theta^v) \bigg)^2
  $$

* For the second case, we apply the following 
  
$$
\begin{split}
m &=  \sum_{i\in N} Q(h_i^t, a_i^t; \theta_i) -Q(h^t,z^t, a^t,\theta^q) + V(h^t,z^t;\theta^v) \\
\mathcal{L}_{\text{nopt}} (\{\theta_i\}_{i\in N}, \theta^v) &= \frac{1}{|\mathcal{B}|}  \sum_{(h^t,z^t,a^t, r^t, h^{t+1}, z^{t+1})\in\mathcal{B}} \min(0,m)^2
\end{split}
$$ 

* QTRAN gas a few limitations
	* It does not scale well due to relying on joint action space 
	* It does not directly enforce the necessary and sufficient conditions for IGM, instead using regularization terms. Hence, IGM is not guaranteed. 
# Links 
* [[MARL Problem Statement]]
* [[MARL from a Game Theoretic Perspective]]
* [[Agent Modeling]] - can also be used for deep learning 

* [[Multi-Agent Reinforcement Learning -- Foundations and Modern Approaches by Albrecht, Christianos and Schafer|Albrecht, Christianos, and Schafer]] - Ch. 9
	* 9.5.2 - derivation of Linear Value Decomposition as well as a proof of it satisfying the IGM property.
	* 9.5.3 - proof of the sufficiency of monotonicity to ensure IGM. 
