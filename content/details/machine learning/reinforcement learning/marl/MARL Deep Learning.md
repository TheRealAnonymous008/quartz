* A naive approach in this regard would be to directly apply the techniques from [[Function Approximation in Reinforcement Learning|Function Approximation]] and [[Policy Gradient Methods|PGMs]] to each agent without making use of joint information. 
* [^Papoudakis_2021] provides a comprehensive empirical comparison for MARL algorithms for Cooperative multi-agent tasks
	* [[MARL Algorithms and Approaches|MARL algorithms]] typically require dense rewards to work properly.  Centralized Training Decentralized Execution seems to be more appropriate for environments with sparse rewards

[^Papoudakis_2021]:  Papoudakis, Christianoos, Schafer, and Albrech (2021) [Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms in Cooperative Tasks](https://arxiv.org/pdf/2006.07869)

# Independent Learning 
## Independent DQN 
* An extension of [[Off Policy Prediction and Control with Approximation#DQN|DQN]] but applied to multiple agents. Here, each agent optimizes the loss function using its local observation history from batch $\mathcal{B}$. 
  
  $$
  \mathcal{L}(\theta) = \frac{1}{|\mathcal{B}|} \sum_{(h^t_i,a^t_i, r^t_i, h^{t+1}_i)\in \mathcal{B}} \bigg(r_i^t +\gamma\max_{a_i\in A_i} Q(h_i^{t+1}, a_i ; \overline\theta_i) - Q(h_i^t, a_i^t; \theta_i ) \bigg)
  $$
* One challenge is that *the replay buffer has to be modified* since agents in IL do not distinguish between the non-stationarity caused by the environment and that by the other agents. 
	* This means that the agent can receive different rewards for performing the same action at the same state 
	* Additionally, because the agents' policies update, the information in the replay buffer can become outdated .
	* This can be solved in a few ways: 
		* By *making the replay buffer smaller* 
		* Using [[Importance Sampling]] to re-weight the experiences in the buffer to account for changing policies and correcting the non-stationarity. 
		* **Hysteretic Q-Learning** uses a smaller learning rate so that decreasing estimates will reduce the stochasticity of the other agent's policies 
		* Use **leniency** ignores decreasing updates of action-value estimates with a given probability that decreases throughout training. 

![[IDQN.png]]
<figcaption> IDQN. Image taken from Albrecht, Christianos and Schafer </figcaption>

## IPGM 
* Compute the gradient with respect to its own policy parameters and then apply the Policy Gradient Theorem 
* *On policy algorithms have an advantage over off-policy approaches since the policy gradient is computed based on recent experiences generated by the current policy*.

![[Independent Reinforce.png]]
<figcaption> Independent REINFORCE. Image taken from Albrecht, Christianos and Schafer</figcaption>

* It can be made asynchronous (such as [[Basic Policy Gradient Methods#A2C - Advantage Actor-Critic|A2C]]). 
![[Independent A2C.png]]
<figcaption> Independent A2C. Image taken from Albrecht, Christianos and Schafer</figcaption>

* We can also use [[Trust Region Policies#PPO|PPO]]. 

# Multi-Agent PGMs 
* The **Multi-Agent Policy Gradient Theorem** extends the regular Policy Gradient Theorem by considering all of the agent's policies. 

$$
\nabla_{\phi_i} J(\theta_i) \propto \mathbb{E}_{\hat{h}\sim P(h\mid \pi), a_i \sim \pi_i, a_{-i}\sim \pi_{-i}} \bigg[Q_i^\pi (\hat{h}, \braket{a_i,a_{-1}}) \nabla _{\phi_i} \log \pi _i \bigg(a_i\mid h_i=\sigma_i(\hat{h}) ; \phi_i\bigg) \bigg]
$$

* We can make use of a **centralized critic**  [^mapgm_1] that is conditioned on information beyond local information. 
	* The centralized critic is optimized using [[Off Policy Prediction and Control with Approximation#Semi-Gradient Methods|Semi-Gradient descent]]. 
	* At minimum, we need to condition the critic on the agent's observation history $h_i^t$. 
	* Any additional information may introduce noise, and increase variance, so it is subject to something akin to the [[Statistical Estimators#Bias-Variance Tradeoff|bias-variance tradeoff]].
	* In practice, we use *state history + local observation history*.
	* This makes it more robust against non-stationarity. 
	* Any [[#Independent Learning]] algorithm can be instantiated with a centralized critic to learn a value function.

![[CA2C.png]]

<figcaption> Centralized A2C. Image taken from Albrecht, Christianos and Schafer </figcaption>

[^mapgm_1]: We don't need to define a decentralized critic since during inference the critic is never used. 
## Action Value Critics and COMA
* *The centralized critic can make use of action-values* $Q$ as well. which is conditioned on local observation history, centralized information  information, and the joint action.
	* The gradient update for the policy then becomes  
	  
	  $$
	  \nabla_{\phi_i} J (\phi_i) = \mathbb{E}_{a^t\sim \pi}\bigg[Q(h_i^t ,z^t, \braket{a_i^t, a_{-i}^t}) \ \nabla_{\phi_i} \log\pi_i (a_i^t \mid h_i^t;\phi_i)\bigg]
	  $$

	* *We make use of on-policy data rather than off-policy data*.
	* In practice, rather than have many action-values for each joint action (with high dimensionality at scale), we instead output the action value given the current joint action (which scales linearly to the number of agents)

* *We can use action-value critics for the multi-agent credit assignment problem* using **difference rewards** to approximate the difference in reward if the agent took action $\bar{a}_i$ instead. We call $\bar{a}_i$ the **default action**
  
  $$
  d_i = R_i(s, \braket{a_i,a_{-i}}) - R_i (s, \bar{a}_i, a_{-i})
  $$
  
	* In practice, determining the default action is difficult, so we use the **aristocratic utility** which measures if the chosen action performs better than a random action sampled from the policy 
	  
	  $$
	  d_i = R(s,\braket{a_i,a_{-i}}) - \mathbb{E}_{a_i'\sim \pi_i} \big[R_i (s, \braket{a_i', a_{-i}}) \big]
	  $$

* **Counterfactual Multi-agent Policy Gradient (COMA)** estimates the advantage using a counterfactual baseline computed using the aristocratic utility. *This enables more efficient computation at the cost of higher variance* 
  
  $$
  \text{Adv}_i(h_i, z, a) = Q(h_i ,z,a;\theta) - \sum_{a_i'\in A_i} \pi (a_i'\mid h_i;\phi_i) \ Q(h_i, z, \braket{a_i',a_{-i}}; \theta) 
  $$

## Pareto-AC
* **Pareto-AC** is an approach where the goal is to learn a Pareto-optimal equilibrium in no-conflict games. This is done by *incorporating the inductive bias that all other agents prefer the same outcome*. That is, assume all other agents follow the policy $\pi_{-i}^+$ which maximizes its reward (or in training, its $Q$-value). That is 
  
  $$
  \pi_{-i}^+ \in \underset{\pi_{-i}}{\text{argmax}} \ U_i(\pi_i,\pi_{-i}) = \underset{a_{-i}}{\text{argmax}} \ Q(h_i^t, z^t, \braket{a_i^t, a_{-i}})
  $$

* The loss function for the policy turns out to be 
  $$
  \mathcal{L}(\phi_i) = -\mathbb{E}_{a'_i \sim \pi_i, a_{-i}^t \sim \pi_{-i}^t} \bigg[\log\pi(a_i^t\mid h_i^t ; \phi_i) \ \bigg(Q^{\pi^+}  (h_i^t, z^t, \braket{a_i^t, a_{-i}^t}; \theta _i^q - V^{\pi^+} (h_i^t, z^t ; \theta_i^v)    \bigg) \bigg]
  $$

* Pareto-AC suffers from inefficiencies as agent count increases

## MAPPO
* **Multi Agent [[Trust Region Policies#PPO|PPO]]**  proposed by [^yu_2021]. PPO-based multi-agent algorithms achieve strong performance for multi-agent testbeds. PPO achieves good final returns with good sample efficiency for common reward games (i.e., cooperative settings)
* The success of MAPPO can, according to [^yu_2021] be attributed to the following:
	* *Value normalization* - by standardizing the value targets using running estimates of the mean and standard deviation of the value targets, instabilities due to value targets are mitigated.
	* *Input representation* - MAPPO blends two approaches to input representation (namely (1) concatenating local observations as global state and (2) global information provided by the environment). It allows the agent to concatenate its local observation state with the global observation state. It is important to make sure that the input dimension does not blow up as a result. 
	* *Training Data Usage*: Empirical tests suggest that MAPPO's performance degrades when samples are reused too often. 
		* This may be because of non-stationarity in MARL -- using fewer epochs per update limits the change in agent policies.
		* Also using more data to estimate gradients typically leads to improved practical performance
		* As recommended by the paper: Use at most $10$ training epochs on difficult environments and $15$ training epochs on easy environments. Additionally, avoid splitting data into mini-batches.
	* *PPO Clipping*: Clipping prevents the policy and value functions from changing too much. It is (hypothesized) to limit non-stationarity.
	* *PPO batch size*: A larger batch generally will result in more accurate gradients, yielding better updates to the value functions and policies.

![[MAPPO.png]]
<figcaption> MAPPO Algorithm. Image taken from Yu et al. (2022) </figcaption>

* In the case of heterogeneous agents, each agent can have its own actor and critic networks.
* The actor objective is to maximize the following
  $$
  L(\theta) = \left[\frac{1}{Bn} \sum_{t=1}^B \sum_{i=1}^n \min\left(r_{\theta,i}^{t} A_i^{t}, \text{clip}(r_{\theta,i}^{t}, 1-\epsilon,1+\epsilon)A_i^{t}\right)\right] + \sigma\frac{1}{Bn}\sum_{t=1}^B\sum_{i=1}^n S[\pi_\theta(o_i^t)]
  $$
  Where $S$ is the policy entropy and $\sigma$ the entropy coefficient parameter. $B$ denotes the batch size and $n$ the number of agents.
  
  Also 
  $$
  r_{\theta,i}^t = \frac{\pi_\theta(a_i^t\mid o_i^t)}{\pi_{\theta_\text{old}} (a_i^t\mid o_i^t)}
  $$
  And also $A_i^t$ is computed using the General Advantage Estimator method

* The critic objective is to maximize the following
  $$
  L(\phi) = \frac{1}{Bn} \sum_{t=1}^B\sum_{i=1}^n \left(\max\left[(V_\phi(s_i^t) - \hat{R}_t)^2 ,\left(\text{clip}(V_\phi(s_i^t), V_{\phi_\text{old}} (s_i^t) - \epsilon, V_{\phi_\text{old}}(s_i^t) + \epsilon \right) - \hat{R}_t^2\right]\right)
  $$
  Where $\hat{R}_t$ is the discounted reward to go

[^Yu_2021]:: Yu, et. al (2022) [The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games](https://arxiv.org/pdf/2103.01955)

# Homogeneous Agents 
## Parameter Sharing 
* A method where *each agent uses the same set of parameter values* in their neural networks. That is, we have either of the following (or both)
  $$
  \begin{split}  
  \theta_{\text{shared}} &= \theta_1 = \dots = \theta_N  \\ 
  \phi_{\text{shared}} &= \phi_1 = \dots = \phi_N  \\ 
  \end{split}
  $$

* This allows for a constant number of parameters regardless of agent count and it also allows for parameters to be updated by a broader set of experiences. 
* *Assumption*: The environment has strongly homogeneous agents. 

* We can get around the strong assumption of strongly homogeneous agents by using **agent indexing** where agents index their observations [^param_1]. In practice, this is not sufficient 

[^param_1]: Analogous to training the models to be few-shot since it encodes many behaviors

## Experience Sharing 
* Each agent has different parameters, but the parameters are *trained on the same set of experiences*. 
* For algorithms that use DQN, we may use a shared replay buffer. 
* *Assumption*: The environment has weakly homogeneous agents. 

* It has the following advantages over parameter sharing :
	* This allows parameters to be trained on more diverse 
	* It retains the flexibility of each agent to adopt its own policy (which has been shown to yield higher returns). 
	* It is more sample efficient 
	* It leads to agents having a uniform learning progression since agents that perform poorly have access to the experiences of the best agents. 
	* Agents can better coordinate and are more efficient, especially at data collection. 

* *This requires storing more experiences in the buffer*. 
* *This is much easier to do with off-policy than on-policy approaches*. On policy approaches require the use of [[Importance Sampling]]. We do this by imposing a loss function as follows 
  
  $$
  \mathcal{L} (\phi) = \frac{\pi (a^t \mid h^t ; \phi)}{\pi_\beta (a^t \mid h^t)} \bigg(r^t + \gamma V(h^{t+1};\theta) -V(h^t;\theta)\bigg) \log\pi(a^t\mid h^t; \phi)
  $$
* We can also weigh the experiences of other agents differently to the current agent. We do this using a hyperparameter $\lambda$.  The loss function is as follows 
  
  $$
  \begin{split}
  L_k^i  &=  \bigg(r^t_k + \gamma V(h^{t+1}_k;\theta_i) -V(h^t_k;\theta_i)\bigg) \log\pi(a^t_k\mid h^t_k; \phi_i) \\ 
  
  \mathcal{L} (\phi_i ) &= -L_i^i - \lambda \sum_{k\ne i} \frac{\pi (a^t_k \mid h^t_k ; \phi_i)}{\pi(a^t_k \mid h^t_k;\phi_k)} \ L_k^i
  \end{split}
  $$



![[DQN with shared experience.png]]
<figcaption> DQN with shared experience replay. Image taken from Albrecht, Christianos  and Schafer  </figcaption>

# Other Techniques 
* [[Agent Modeling]] - can also be used for deep learning so that each agent has a model of the other agents in the environment 
* [[Decision Time Planning]] - approaches such as MCTS can be generalized to a deep-learning setting. 
* [[Value Reward Decomposition]] - approaches that involve decomposing the joint value reward function to something simpler (i.e., based on the local value reward function for each agent) 
* [[Mean Field MARL]] 


# Links 
* [[MARL Problem Statement]]
* [[MARL from a Game Theoretic Perspective]]

* [[Multi-Agent Reinforcement Learning -- Foundations and Modern Approaches by Albrecht, Christianos and Schafer|Albrecht, Christianos, and Schafer]] - Ch. 9
	* 9.5.3 - proof of the sufficiency of monotonicity to ensure IGM. 
