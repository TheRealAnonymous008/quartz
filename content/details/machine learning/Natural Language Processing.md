* Transformer Models are programmable. You can provide them with enough context (i.e., Memory), possibly from a data store (i.e., Virtual Memory). They then perform some processing and give you a response. For more info on prompt engineering see [this](https://www.oneusefulthing.org/p/a-guide-to-prompting-ai-for-what)
 
* Transformers are so effective because learning from human language is a cheat. *Human [[Linguistics|language]] encodes [[System Opportunities#Systems Thinking|human thinking]], and transformers learn to exploit this. 
# Topics
* [[Sequence Based Algorithms]]
* [[The Transformer Model]]
* [[Language Models]]
* [[Large Language Models]]