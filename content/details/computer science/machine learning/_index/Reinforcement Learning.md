* Reinforcement Learning can be thought of as an adversarial process between generating a good policy based on a value function and generating a value function from a given policy. Both converge to the optimum.
# Topics
* [[The Setting for Reinforcement Learning]] - gives context to the Reinforcement Learning Problem
* [[A Unified View on Reinforcement Learning Approaches]] - a broad overview on RL methods.
* [[The Exploitation-Exploration Trade-Off]] - a trade-off central to RL.
* [[Markov Processes in Machine Learning]] - perhaps the most common way to frame a reinforcement learning task. See also MDP Control for how optimal policies behave.
* [[Dynamic Programming for Reinforcement Learning]] - for Dynamic Programming based methods on handling RL tasks.
* [[Monte Carlo Methods in Reinforcement Learning]] - for methods based on partial information about the environment.
* [[Temporal Difference Learning]] - for methods that combine DP (bootstrapping) and Monte Carlo (by learning from the environment)
* [[Eligibility Traces]] - generalizations of Temporal Difference Learning that involve giving a certain amount of credit to each state for the received return.
* [[A Unified View on Planning and Learning]] - integrating dynamic programming techniques with Monte Carlo and Eligibility-traces.
* [[Decision Time Planning]] - RL-adjacent algorithms that focus on applying MC techniques to tree search.
* [[Function Approximation in Reinforcement Learning]] - how can we generalize experience with a limited subset of the state space? 
# Papers
* Branching Reinforcement Learning by Du, and Chen (Jun 15, 2022) 
# Links
